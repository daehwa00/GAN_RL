{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/daehwa/Documents/MINST_GAN/MNIST_GAN.ipynb 셀 3\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/daehwa/Documents/MINST_GAN/MNIST_GAN.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m noise_dim \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/daehwa/Documents/MINST_GAN/MNIST_GAN.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Data loader\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/daehwa/Documents/MINST_GAN/MNIST_GAN.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([transforms\u001b[39m.\u001b[39mToTensor(), transforms\u001b[39m.\u001b[39mNormalize((\u001b[39m0.5\u001b[39m,), (\u001b[39m0.5\u001b[39m,))])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/daehwa/Documents/MINST_GAN/MNIST_GAN.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(datasets\u001b[39m.\u001b[39mMNIST(\u001b[39m'\u001b[39m\u001b[39m./data\u001b[39m\u001b[39m'\u001b[39m, train\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, download\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, transform\u001b[39m=\u001b[39mtransform), batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 3000\n",
    "lr = 0.0002\n",
    "epochs = 50\n",
    "noise_dim = 100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "generator = Generator(noise_dim, 784)\n",
    "discriminator = Discriminator(784)\n",
    "\n",
    "# Loss and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [200/938], d_loss: 0.2706, g_loss: 2.5051, D(x): 1.00, D(G(z)): 0.23\n",
      "Epoch [1/50], Step [400/938], d_loss: 0.2675, g_loss: 10.0963, D(x): 0.89, D(G(z)): 0.01\n",
      "Epoch [1/50], Step [600/938], d_loss: 0.0336, g_loss: 9.6138, D(x): 0.99, D(G(z)): 0.02\n",
      "Epoch [1/50], Step [800/938], d_loss: 1.1722, g_loss: 0.9607, D(x): 0.59, D(G(z)): 0.45\n",
      "Epoch [2/50], Step [200/938], d_loss: 1.0271, g_loss: 1.5888, D(x): 0.75, D(G(z)): 0.36\n",
      "Epoch [2/50], Step [400/938], d_loss: 1.1496, g_loss: 2.2873, D(x): 0.64, D(G(z)): 0.24\n",
      "Epoch [2/50], Step [600/938], d_loss: 1.3171, g_loss: 1.6597, D(x): 0.70, D(G(z)): 0.46\n",
      "Epoch [2/50], Step [800/938], d_loss: 0.3081, g_loss: 2.3326, D(x): 0.95, D(G(z)): 0.17\n",
      "Epoch [3/50], Step [200/938], d_loss: 0.7673, g_loss: 2.4993, D(x): 0.72, D(G(z)): 0.15\n",
      "Epoch [3/50], Step [400/938], d_loss: 0.0604, g_loss: 5.3336, D(x): 0.98, D(G(z)): 0.03\n",
      "Epoch [3/50], Step [600/938], d_loss: 0.1565, g_loss: 4.5396, D(x): 0.92, D(G(z)): 0.02\n",
      "Epoch [3/50], Step [800/938], d_loss: 0.3555, g_loss: 3.2779, D(x): 0.93, D(G(z)): 0.17\n",
      "Epoch [4/50], Step [200/938], d_loss: 0.3698, g_loss: 3.9917, D(x): 0.87, D(G(z)): 0.04\n",
      "Epoch [4/50], Step [400/938], d_loss: 0.2667, g_loss: 3.6040, D(x): 0.92, D(G(z)): 0.09\n",
      "Epoch [4/50], Step [600/938], d_loss: 0.1858, g_loss: 3.8682, D(x): 0.96, D(G(z)): 0.10\n",
      "Epoch [4/50], Step [800/938], d_loss: 0.2627, g_loss: 4.0972, D(x): 1.00, D(G(z)): 0.18\n",
      "Epoch [5/50], Step [200/938], d_loss: 0.4006, g_loss: 3.6414, D(x): 0.85, D(G(z)): 0.07\n",
      "Epoch [5/50], Step [400/938], d_loss: 0.2153, g_loss: 3.4622, D(x): 0.94, D(G(z)): 0.11\n",
      "Epoch [5/50], Step [600/938], d_loss: 0.3255, g_loss: 3.5225, D(x): 0.91, D(G(z)): 0.12\n",
      "Epoch [5/50], Step [800/938], d_loss: 0.2856, g_loss: 4.6129, D(x): 0.88, D(G(z)): 0.03\n",
      "Epoch [6/50], Step [200/938], d_loss: 0.4692, g_loss: 3.5373, D(x): 0.91, D(G(z)): 0.17\n",
      "Epoch [6/50], Step [400/938], d_loss: 0.2104, g_loss: 3.8841, D(x): 0.94, D(G(z)): 0.08\n",
      "Epoch [6/50], Step [600/938], d_loss: 0.2096, g_loss: 3.8276, D(x): 0.93, D(G(z)): 0.07\n",
      "Epoch [6/50], Step [800/938], d_loss: 0.2617, g_loss: 3.7731, D(x): 0.94, D(G(z)): 0.09\n",
      "Epoch [7/50], Step [200/938], d_loss: 0.4041, g_loss: 3.9369, D(x): 0.89, D(G(z)): 0.10\n",
      "Epoch [7/50], Step [400/938], d_loss: 0.2102, g_loss: 4.0048, D(x): 0.94, D(G(z)): 0.07\n",
      "Epoch [7/50], Step [600/938], d_loss: 0.2424, g_loss: 3.9318, D(x): 0.91, D(G(z)): 0.06\n",
      "Epoch [7/50], Step [800/938], d_loss: 0.1809, g_loss: 4.2512, D(x): 0.93, D(G(z)): 0.05\n",
      "Epoch [8/50], Step [200/938], d_loss: 0.3661, g_loss: 5.0746, D(x): 0.85, D(G(z)): 0.04\n",
      "Epoch [8/50], Step [400/938], d_loss: 0.2716, g_loss: 3.6123, D(x): 0.93, D(G(z)): 0.11\n",
      "Epoch [8/50], Step [600/938], d_loss: 0.7034, g_loss: 2.5062, D(x): 0.96, D(G(z)): 0.35\n",
      "Epoch [8/50], Step [800/938], d_loss: 0.2227, g_loss: 3.3969, D(x): 0.96, D(G(z)): 0.13\n",
      "Epoch [9/50], Step [200/938], d_loss: 0.4695, g_loss: 3.2763, D(x): 0.92, D(G(z)): 0.19\n",
      "Epoch [9/50], Step [400/938], d_loss: 0.4397, g_loss: 2.8332, D(x): 0.83, D(G(z)): 0.08\n",
      "Epoch [9/50], Step [600/938], d_loss: 0.5942, g_loss: 2.8766, D(x): 0.78, D(G(z)): 0.11\n",
      "Epoch [9/50], Step [800/938], d_loss: 0.6741, g_loss: 1.9843, D(x): 0.78, D(G(z)): 0.15\n",
      "Epoch [10/50], Step [200/938], d_loss: 0.3956, g_loss: 2.7746, D(x): 0.87, D(G(z)): 0.12\n",
      "Epoch [10/50], Step [400/938], d_loss: 0.5964, g_loss: 2.6277, D(x): 0.83, D(G(z)): 0.20\n",
      "Epoch [10/50], Step [600/938], d_loss: 0.7141, g_loss: 1.7278, D(x): 0.81, D(G(z)): 0.24\n",
      "Epoch [10/50], Step [800/938], d_loss: 0.5073, g_loss: 2.8660, D(x): 0.88, D(G(z)): 0.18\n",
      "Epoch [11/50], Step [200/938], d_loss: 0.4129, g_loss: 3.0162, D(x): 0.87, D(G(z)): 0.15\n",
      "Epoch [11/50], Step [400/938], d_loss: 0.5154, g_loss: 2.9871, D(x): 0.83, D(G(z)): 0.11\n",
      "Epoch [11/50], Step [600/938], d_loss: 0.4947, g_loss: 2.2446, D(x): 0.85, D(G(z)): 0.20\n",
      "Epoch [11/50], Step [800/938], d_loss: 0.5331, g_loss: 2.6440, D(x): 0.82, D(G(z)): 0.13\n",
      "Epoch [12/50], Step [200/938], d_loss: 0.5207, g_loss: 2.9900, D(x): 0.83, D(G(z)): 0.12\n",
      "Epoch [12/50], Step [400/938], d_loss: 0.5242, g_loss: 2.0375, D(x): 0.85, D(G(z)): 0.22\n",
      "Epoch [12/50], Step [600/938], d_loss: 0.4773, g_loss: 2.7530, D(x): 0.88, D(G(z)): 0.16\n",
      "Epoch [12/50], Step [800/938], d_loss: 0.5307, g_loss: 1.7004, D(x): 0.89, D(G(z)): 0.26\n",
      "Epoch [13/50], Step [200/938], d_loss: 0.8305, g_loss: 1.8690, D(x): 0.77, D(G(z)): 0.23\n",
      "Epoch [13/50], Step [400/938], d_loss: 0.3847, g_loss: 3.4453, D(x): 0.83, D(G(z)): 0.07\n",
      "Epoch [13/50], Step [600/938], d_loss: 0.5472, g_loss: 1.9945, D(x): 0.85, D(G(z)): 0.22\n",
      "Epoch [13/50], Step [800/938], d_loss: 0.6529, g_loss: 1.5840, D(x): 0.87, D(G(z)): 0.30\n",
      "Epoch [14/50], Step [200/938], d_loss: 0.5710, g_loss: 2.3251, D(x): 0.84, D(G(z)): 0.19\n",
      "Epoch [14/50], Step [400/938], d_loss: 0.4168, g_loss: 2.6016, D(x): 0.87, D(G(z)): 0.13\n",
      "Epoch [14/50], Step [600/938], d_loss: 0.5279, g_loss: 1.8081, D(x): 0.83, D(G(z)): 0.22\n",
      "Epoch [14/50], Step [800/938], d_loss: 0.4941, g_loss: 2.8330, D(x): 0.80, D(G(z)): 0.09\n",
      "Epoch [15/50], Step [200/938], d_loss: 0.8645, g_loss: 2.0081, D(x): 0.72, D(G(z)): 0.17\n",
      "Epoch [15/50], Step [400/938], d_loss: 0.6019, g_loss: 1.6279, D(x): 0.83, D(G(z)): 0.24\n",
      "Epoch [15/50], Step [600/938], d_loss: 1.0286, g_loss: 1.5994, D(x): 0.86, D(G(z)): 0.39\n",
      "Epoch [15/50], Step [800/938], d_loss: 0.4037, g_loss: 2.4648, D(x): 0.87, D(G(z)): 0.15\n",
      "Epoch [16/50], Step [200/938], d_loss: 0.7132, g_loss: 1.5997, D(x): 0.83, D(G(z)): 0.28\n",
      "Epoch [16/50], Step [400/938], d_loss: 0.7606, g_loss: 1.5563, D(x): 0.82, D(G(z)): 0.26\n",
      "Epoch [16/50], Step [600/938], d_loss: 0.9755, g_loss: 2.8645, D(x): 0.64, D(G(z)): 0.14\n",
      "Epoch [16/50], Step [800/938], d_loss: 0.5614, g_loss: 1.9176, D(x): 0.84, D(G(z)): 0.24\n",
      "Epoch [17/50], Step [200/938], d_loss: 0.4999, g_loss: 2.2918, D(x): 0.82, D(G(z)): 0.12\n",
      "Epoch [17/50], Step [400/938], d_loss: 0.8208, g_loss: 1.5534, D(x): 0.73, D(G(z)): 0.26\n",
      "Epoch [17/50], Step [600/938], d_loss: 0.7214, g_loss: 1.4227, D(x): 0.73, D(G(z)): 0.24\n",
      "Epoch [17/50], Step [800/938], d_loss: 0.8104, g_loss: 1.3288, D(x): 0.85, D(G(z)): 0.34\n",
      "Epoch [18/50], Step [200/938], d_loss: 0.5995, g_loss: 2.0918, D(x): 0.81, D(G(z)): 0.21\n",
      "Epoch [18/50], Step [400/938], d_loss: 0.5699, g_loss: 2.4413, D(x): 0.78, D(G(z)): 0.15\n",
      "Epoch [18/50], Step [600/938], d_loss: 0.4954, g_loss: 2.2856, D(x): 0.88, D(G(z)): 0.26\n",
      "Epoch [18/50], Step [800/938], d_loss: 0.6866, g_loss: 1.6696, D(x): 0.79, D(G(z)): 0.26\n",
      "Epoch [19/50], Step [200/938], d_loss: 0.5903, g_loss: 2.3112, D(x): 0.82, D(G(z)): 0.23\n",
      "Epoch [19/50], Step [400/938], d_loss: 0.8850, g_loss: 1.6440, D(x): 0.76, D(G(z)): 0.28\n",
      "Epoch [19/50], Step [600/938], d_loss: 0.8011, g_loss: 1.7669, D(x): 0.80, D(G(z)): 0.31\n",
      "Epoch [19/50], Step [800/938], d_loss: 0.8797, g_loss: 1.4800, D(x): 0.77, D(G(z)): 0.33\n",
      "Epoch [20/50], Step [200/938], d_loss: 0.5085, g_loss: 1.8177, D(x): 0.83, D(G(z)): 0.17\n",
      "Epoch [20/50], Step [400/938], d_loss: 0.9701, g_loss: 1.5667, D(x): 0.72, D(G(z)): 0.27\n",
      "Epoch [20/50], Step [600/938], d_loss: 0.8397, g_loss: 2.0455, D(x): 0.80, D(G(z)): 0.27\n",
      "Epoch [20/50], Step [800/938], d_loss: 0.7392, g_loss: 1.7339, D(x): 0.77, D(G(z)): 0.27\n",
      "Epoch [21/50], Step [200/938], d_loss: 0.9262, g_loss: 1.1541, D(x): 0.69, D(G(z)): 0.33\n",
      "Epoch [21/50], Step [400/938], d_loss: 1.0925, g_loss: 1.1964, D(x): 0.71, D(G(z)): 0.37\n",
      "Epoch [21/50], Step [600/938], d_loss: 0.8473, g_loss: 1.6853, D(x): 0.74, D(G(z)): 0.28\n",
      "Epoch [21/50], Step [800/938], d_loss: 0.8707, g_loss: 1.4633, D(x): 0.73, D(G(z)): 0.32\n",
      "Epoch [22/50], Step [200/938], d_loss: 1.0227, g_loss: 1.6631, D(x): 0.64, D(G(z)): 0.27\n",
      "Epoch [22/50], Step [400/938], d_loss: 1.3096, g_loss: 1.6985, D(x): 0.52, D(G(z)): 0.22\n",
      "Epoch [22/50], Step [600/938], d_loss: 0.8584, g_loss: 1.4907, D(x): 0.71, D(G(z)): 0.26\n",
      "Epoch [22/50], Step [800/938], d_loss: 0.6414, g_loss: 2.1557, D(x): 0.76, D(G(z)): 0.17\n",
      "Epoch [23/50], Step [200/938], d_loss: 0.7601, g_loss: 1.6657, D(x): 0.74, D(G(z)): 0.21\n",
      "Epoch [23/50], Step [400/938], d_loss: 0.7732, g_loss: 1.6893, D(x): 0.73, D(G(z)): 0.22\n",
      "Epoch [23/50], Step [600/938], d_loss: 0.8550, g_loss: 1.6939, D(x): 0.68, D(G(z)): 0.26\n",
      "Epoch [23/50], Step [800/938], d_loss: 0.7875, g_loss: 1.7045, D(x): 0.77, D(G(z)): 0.25\n",
      "Epoch [24/50], Step [200/938], d_loss: 0.6807, g_loss: 1.8503, D(x): 0.73, D(G(z)): 0.19\n",
      "Epoch [24/50], Step [400/938], d_loss: 0.6471, g_loss: 1.6953, D(x): 0.79, D(G(z)): 0.28\n",
      "Epoch [24/50], Step [600/938], d_loss: 0.7804, g_loss: 1.7506, D(x): 0.78, D(G(z)): 0.30\n",
      "Epoch [24/50], Step [800/938], d_loss: 1.0890, g_loss: 1.7584, D(x): 0.61, D(G(z)): 0.26\n",
      "Epoch [25/50], Step [200/938], d_loss: 0.7218, g_loss: 1.9105, D(x): 0.79, D(G(z)): 0.25\n",
      "Epoch [25/50], Step [400/938], d_loss: 1.0377, g_loss: 1.3551, D(x): 0.69, D(G(z)): 0.35\n",
      "Epoch [25/50], Step [600/938], d_loss: 0.8820, g_loss: 1.4052, D(x): 0.78, D(G(z)): 0.33\n",
      "Epoch [25/50], Step [800/938], d_loss: 0.7040, g_loss: 1.7314, D(x): 0.73, D(G(z)): 0.23\n",
      "Epoch [26/50], Step [200/938], d_loss: 1.2725, g_loss: 2.0022, D(x): 0.53, D(G(z)): 0.24\n",
      "Epoch [26/50], Step [400/938], d_loss: 0.9418, g_loss: 1.4703, D(x): 0.61, D(G(z)): 0.22\n",
      "Epoch [26/50], Step [600/938], d_loss: 0.8664, g_loss: 1.5267, D(x): 0.68, D(G(z)): 0.25\n",
      "Epoch [26/50], Step [800/938], d_loss: 0.8013, g_loss: 1.6155, D(x): 0.69, D(G(z)): 0.25\n",
      "Epoch [27/50], Step [200/938], d_loss: 0.7483, g_loss: 1.7462, D(x): 0.77, D(G(z)): 0.29\n",
      "Epoch [27/50], Step [400/938], d_loss: 1.0154, g_loss: 1.3031, D(x): 0.69, D(G(z)): 0.34\n",
      "Epoch [27/50], Step [600/938], d_loss: 0.9268, g_loss: 1.4223, D(x): 0.72, D(G(z)): 0.33\n",
      "Epoch [27/50], Step [800/938], d_loss: 0.7066, g_loss: 1.6007, D(x): 0.77, D(G(z)): 0.27\n",
      "Epoch [28/50], Step [200/938], d_loss: 0.6880, g_loss: 1.6491, D(x): 0.76, D(G(z)): 0.26\n",
      "Epoch [28/50], Step [400/938], d_loss: 0.7438, g_loss: 1.7814, D(x): 0.72, D(G(z)): 0.24\n",
      "Epoch [28/50], Step [600/938], d_loss: 1.0155, g_loss: 1.3163, D(x): 0.71, D(G(z)): 0.34\n",
      "Epoch [28/50], Step [800/938], d_loss: 0.8040, g_loss: 1.3622, D(x): 0.74, D(G(z)): 0.32\n",
      "Epoch [29/50], Step [200/938], d_loss: 0.9780, g_loss: 1.1999, D(x): 0.72, D(G(z)): 0.35\n",
      "Epoch [29/50], Step [400/938], d_loss: 1.1035, g_loss: 1.1614, D(x): 0.73, D(G(z)): 0.40\n",
      "Epoch [29/50], Step [600/938], d_loss: 1.0461, g_loss: 1.4570, D(x): 0.64, D(G(z)): 0.29\n",
      "Epoch [29/50], Step [800/938], d_loss: 0.7819, g_loss: 1.4301, D(x): 0.74, D(G(z)): 0.32\n",
      "Epoch [30/50], Step [200/938], d_loss: 1.0285, g_loss: 1.6558, D(x): 0.66, D(G(z)): 0.30\n",
      "Epoch [30/50], Step [400/938], d_loss: 0.7060, g_loss: 1.4985, D(x): 0.81, D(G(z)): 0.31\n",
      "Epoch [30/50], Step [600/938], d_loss: 1.1038, g_loss: 1.5814, D(x): 0.58, D(G(z)): 0.29\n",
      "Epoch [30/50], Step [800/938], d_loss: 0.9950, g_loss: 1.2104, D(x): 0.74, D(G(z)): 0.38\n",
      "Epoch [31/50], Step [200/938], d_loss: 1.3406, g_loss: 0.9200, D(x): 0.69, D(G(z)): 0.47\n",
      "Epoch [31/50], Step [400/938], d_loss: 0.8162, g_loss: 1.6285, D(x): 0.73, D(G(z)): 0.27\n",
      "Epoch [31/50], Step [600/938], d_loss: 0.8629, g_loss: 2.0409, D(x): 0.69, D(G(z)): 0.24\n",
      "Epoch [31/50], Step [800/938], d_loss: 0.9000, g_loss: 2.0197, D(x): 0.60, D(G(z)): 0.18\n",
      "Epoch [32/50], Step [200/938], d_loss: 1.2256, g_loss: 1.3837, D(x): 0.56, D(G(z)): 0.34\n",
      "Epoch [32/50], Step [400/938], d_loss: 0.8928, g_loss: 1.5490, D(x): 0.68, D(G(z)): 0.26\n",
      "Epoch [32/50], Step [600/938], d_loss: 0.9683, g_loss: 1.3268, D(x): 0.67, D(G(z)): 0.32\n",
      "Epoch [32/50], Step [800/938], d_loss: 1.0973, g_loss: 1.8813, D(x): 0.62, D(G(z)): 0.23\n",
      "Epoch [33/50], Step [200/938], d_loss: 1.0004, g_loss: 1.2142, D(x): 0.69, D(G(z)): 0.36\n",
      "Epoch [33/50], Step [400/938], d_loss: 1.2062, g_loss: 1.2518, D(x): 0.59, D(G(z)): 0.39\n",
      "Epoch [33/50], Step [600/938], d_loss: 1.0347, g_loss: 1.2500, D(x): 0.66, D(G(z)): 0.38\n",
      "Epoch [33/50], Step [800/938], d_loss: 1.2181, g_loss: 1.1034, D(x): 0.62, D(G(z)): 0.41\n",
      "Epoch [34/50], Step [200/938], d_loss: 1.0068, g_loss: 1.2575, D(x): 0.65, D(G(z)): 0.36\n",
      "Epoch [34/50], Step [400/938], d_loss: 0.9024, g_loss: 1.8396, D(x): 0.66, D(G(z)): 0.25\n",
      "Epoch [34/50], Step [600/938], d_loss: 1.1372, g_loss: 1.0878, D(x): 0.57, D(G(z)): 0.36\n",
      "Epoch [34/50], Step [800/938], d_loss: 0.7853, g_loss: 1.5730, D(x): 0.69, D(G(z)): 0.23\n",
      "Epoch [35/50], Step [200/938], d_loss: 0.8465, g_loss: 1.5552, D(x): 0.71, D(G(z)): 0.28\n",
      "Epoch [35/50], Step [400/938], d_loss: 1.0128, g_loss: 1.2414, D(x): 0.65, D(G(z)): 0.33\n",
      "Epoch [35/50], Step [600/938], d_loss: 1.1826, g_loss: 1.3192, D(x): 0.62, D(G(z)): 0.39\n",
      "Epoch [35/50], Step [800/938], d_loss: 1.3320, g_loss: 1.1495, D(x): 0.59, D(G(z)): 0.41\n",
      "Epoch [36/50], Step [200/938], d_loss: 0.9397, g_loss: 1.4885, D(x): 0.68, D(G(z)): 0.32\n",
      "Epoch [36/50], Step [400/938], d_loss: 1.3288, g_loss: 1.1752, D(x): 0.64, D(G(z)): 0.44\n",
      "Epoch [36/50], Step [600/938], d_loss: 0.8953, g_loss: 1.6665, D(x): 0.69, D(G(z)): 0.30\n",
      "Epoch [36/50], Step [800/938], d_loss: 1.0041, g_loss: 1.2182, D(x): 0.63, D(G(z)): 0.33\n",
      "Epoch [37/50], Step [200/938], d_loss: 1.0027, g_loss: 1.0952, D(x): 0.66, D(G(z)): 0.35\n",
      "Epoch [37/50], Step [400/938], d_loss: 1.1081, g_loss: 1.1971, D(x): 0.61, D(G(z)): 0.34\n",
      "Epoch [37/50], Step [600/938], d_loss: 0.8281, g_loss: 1.6885, D(x): 0.69, D(G(z)): 0.27\n",
      "Epoch [37/50], Step [800/938], d_loss: 1.1722, g_loss: 0.9360, D(x): 0.65, D(G(z)): 0.44\n",
      "Epoch [38/50], Step [200/938], d_loss: 1.1087, g_loss: 0.8974, D(x): 0.64, D(G(z)): 0.40\n",
      "Epoch [38/50], Step [400/938], d_loss: 1.2241, g_loss: 1.1151, D(x): 0.61, D(G(z)): 0.37\n",
      "Epoch [38/50], Step [600/938], d_loss: 1.0474, g_loss: 1.5032, D(x): 0.66, D(G(z)): 0.36\n",
      "Epoch [38/50], Step [800/938], d_loss: 0.9075, g_loss: 1.2467, D(x): 0.69, D(G(z)): 0.34\n",
      "Epoch [39/50], Step [200/938], d_loss: 1.0673, g_loss: 1.3175, D(x): 0.61, D(G(z)): 0.34\n",
      "Epoch [39/50], Step [400/938], d_loss: 1.2998, g_loss: 1.0362, D(x): 0.66, D(G(z)): 0.48\n",
      "Epoch [39/50], Step [600/938], d_loss: 1.2026, g_loss: 1.2638, D(x): 0.64, D(G(z)): 0.36\n",
      "Epoch [39/50], Step [800/938], d_loss: 1.2853, g_loss: 1.3352, D(x): 0.55, D(G(z)): 0.30\n",
      "Epoch [40/50], Step [200/938], d_loss: 1.2242, g_loss: 1.1816, D(x): 0.52, D(G(z)): 0.33\n",
      "Epoch [40/50], Step [400/938], d_loss: 1.1990, g_loss: 1.1549, D(x): 0.69, D(G(z)): 0.41\n",
      "Epoch [40/50], Step [600/938], d_loss: 1.0367, g_loss: 1.2186, D(x): 0.72, D(G(z)): 0.38\n",
      "Epoch [40/50], Step [800/938], d_loss: 0.8548, g_loss: 2.0054, D(x): 0.62, D(G(z)): 0.21\n",
      "Epoch [41/50], Step [200/938], d_loss: 1.1517, g_loss: 1.5217, D(x): 0.61, D(G(z)): 0.33\n",
      "Epoch [41/50], Step [400/938], d_loss: 1.0714, g_loss: 1.3805, D(x): 0.65, D(G(z)): 0.36\n",
      "Epoch [41/50], Step [600/938], d_loss: 0.9290, g_loss: 1.3784, D(x): 0.69, D(G(z)): 0.31\n",
      "Epoch [41/50], Step [800/938], d_loss: 1.2029, g_loss: 1.1856, D(x): 0.60, D(G(z)): 0.40\n",
      "Epoch [42/50], Step [200/938], d_loss: 1.0659, g_loss: 1.2447, D(x): 0.60, D(G(z)): 0.32\n",
      "Epoch [42/50], Step [400/938], d_loss: 1.3096, g_loss: 1.0831, D(x): 0.55, D(G(z)): 0.40\n",
      "Epoch [42/50], Step [600/938], d_loss: 0.9262, g_loss: 1.6011, D(x): 0.65, D(G(z)): 0.26\n",
      "Epoch [42/50], Step [800/938], d_loss: 1.3838, g_loss: 0.8988, D(x): 0.55, D(G(z)): 0.44\n",
      "Epoch [43/50], Step [200/938], d_loss: 1.2925, g_loss: 0.9703, D(x): 0.59, D(G(z)): 0.44\n",
      "Epoch [43/50], Step [400/938], d_loss: 1.0830, g_loss: 1.4518, D(x): 0.59, D(G(z)): 0.30\n",
      "Epoch [43/50], Step [600/938], d_loss: 1.0484, g_loss: 1.1835, D(x): 0.62, D(G(z)): 0.35\n",
      "Epoch [43/50], Step [800/938], d_loss: 1.1242, g_loss: 1.0741, D(x): 0.62, D(G(z)): 0.38\n",
      "Epoch [44/50], Step [200/938], d_loss: 1.2110, g_loss: 1.1457, D(x): 0.59, D(G(z)): 0.39\n",
      "Epoch [44/50], Step [400/938], d_loss: 1.1958, g_loss: 1.2845, D(x): 0.56, D(G(z)): 0.33\n",
      "Epoch [44/50], Step [600/938], d_loss: 0.9076, g_loss: 1.5138, D(x): 0.68, D(G(z)): 0.31\n",
      "Epoch [44/50], Step [800/938], d_loss: 1.0113, g_loss: 1.1533, D(x): 0.64, D(G(z)): 0.33\n",
      "Epoch [45/50], Step [200/938], d_loss: 1.2172, g_loss: 1.6979, D(x): 0.56, D(G(z)): 0.32\n",
      "Epoch [45/50], Step [400/938], d_loss: 1.1578, g_loss: 1.1528, D(x): 0.63, D(G(z)): 0.39\n",
      "Epoch [45/50], Step [600/938], d_loss: 1.0204, g_loss: 1.2627, D(x): 0.69, D(G(z)): 0.35\n",
      "Epoch [45/50], Step [800/938], d_loss: 1.0342, g_loss: 1.2659, D(x): 0.65, D(G(z)): 0.37\n",
      "Epoch [46/50], Step [200/938], d_loss: 1.1534, g_loss: 1.1711, D(x): 0.60, D(G(z)): 0.36\n",
      "Epoch [46/50], Step [400/938], d_loss: 0.9195, g_loss: 1.3527, D(x): 0.66, D(G(z)): 0.32\n",
      "Epoch [46/50], Step [600/938], d_loss: 1.0672, g_loss: 1.1370, D(x): 0.70, D(G(z)): 0.41\n",
      "Epoch [46/50], Step [800/938], d_loss: 1.1905, g_loss: 1.0594, D(x): 0.60, D(G(z)): 0.41\n",
      "Epoch [47/50], Step [200/938], d_loss: 1.1073, g_loss: 1.0319, D(x): 0.60, D(G(z)): 0.38\n",
      "Epoch [47/50], Step [400/938], d_loss: 1.0361, g_loss: 1.1884, D(x): 0.60, D(G(z)): 0.35\n",
      "Epoch [47/50], Step [600/938], d_loss: 0.9678, g_loss: 1.1635, D(x): 0.67, D(G(z)): 0.35\n",
      "Epoch [47/50], Step [800/938], d_loss: 1.1678, g_loss: 1.0848, D(x): 0.56, D(G(z)): 0.38\n",
      "Epoch [48/50], Step [200/938], d_loss: 0.9439, g_loss: 1.3685, D(x): 0.72, D(G(z)): 0.36\n",
      "Epoch [48/50], Step [400/938], d_loss: 1.1709, g_loss: 1.2171, D(x): 0.63, D(G(z)): 0.40\n",
      "Epoch [48/50], Step [600/938], d_loss: 1.0642, g_loss: 1.0600, D(x): 0.60, D(G(z)): 0.34\n",
      "Epoch [48/50], Step [800/938], d_loss: 1.0843, g_loss: 1.1224, D(x): 0.61, D(G(z)): 0.37\n",
      "Epoch [49/50], Step [200/938], d_loss: 1.2923, g_loss: 1.0204, D(x): 0.61, D(G(z)): 0.45\n",
      "Epoch [49/50], Step [400/938], d_loss: 1.0797, g_loss: 1.4898, D(x): 0.66, D(G(z)): 0.36\n",
      "Epoch [49/50], Step [600/938], d_loss: 0.9346, g_loss: 1.3238, D(x): 0.65, D(G(z)): 0.34\n",
      "Epoch [49/50], Step [800/938], d_loss: 0.9810, g_loss: 1.1257, D(x): 0.67, D(G(z)): 0.35\n",
      "Epoch [50/50], Step [200/938], d_loss: 0.8779, g_loss: 1.5085, D(x): 0.68, D(G(z)): 0.31\n",
      "Epoch [50/50], Step [400/938], d_loss: 1.0898, g_loss: 0.9971, D(x): 0.64, D(G(z)): 0.41\n",
      "Epoch [50/50], Step [600/938], d_loss: 1.0673, g_loss: 1.2554, D(x): 0.59, D(G(z)): 0.33\n",
      "Epoch [50/50], Step [800/938], d_loss: 1.1387, g_loss: 1.1954, D(x): 0.58, D(G(z)): 0.31\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for batch_idx, (real_images, _) in enumerate(train_loader):\n",
    "        batch_size = real_images.size(0)\n",
    "        \n",
    "        # Real and fake labels\n",
    "        real_labels = torch.ones(batch_size, 1)\n",
    "        fake_labels = torch.zeros(batch_size, 1)\n",
    "        \n",
    "        # Train discriminator\n",
    "        outputs = discriminator(real_images)\n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "        real_score = outputs\n",
    "        \n",
    "        noise = torch.randn(batch_size, noise_dim)\n",
    "        fake_images = generator(noise)\n",
    "        outputs = discriminator(fake_images.detach())\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "        \n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # Train generator\n",
    "        outputs = discriminator(fake_images)\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        \n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        if (batch_idx+1) % 200 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Step [{batch_idx+1}/{len(train_loader)}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}, D(x): {real_score.mean().item():.2f}, D(G(z)): {fake_score.mean().item():.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate images from noise, using the generator network.\n",
    "with torch.no_grad():\n",
    "    noise = torch.randn(batch_size, noise_dim)\n",
    "    generated_images = generator(noise).cpu().view(-1, 28, 28)\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(real_images[0].view(-1, 28, 28).numpy(), (1,2,0)))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(generated_images[0].view(-1, 28, 28).numpy(), (1,2,0)))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 3000\n",
    "lr = 0.0002\n",
    "epochs = 50\n",
    "noise_dim = 100\n",
    "\n",
    "# Data loader\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_loader = DataLoader(datasets.MNIST('./data', train=True, download=True, transform=transform), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(datasets.MNIST('./data', train=False, download=True, transform=transform), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Generator Model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 28 * 28, 512)\n",
    "        \n",
    "        self.action_mean = nn.Linear(512, 3)  # For x, y, brightness\n",
    "        self.action_std = nn.Linear(512, 3)  # For x, y, brightness\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = x.view(-1, 64 * 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        \n",
    "        action_mean = self.action_mean(x)\n",
    "        action_std = self.action_std(x)\n",
    "        action_std = nn.Softplus()(action_std)  # Ensure standard deviation is positive\n",
    "        \n",
    "        return action_mean, action_std\n",
    "    \n",
    "# Discriminator Model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(32 * 28 * 28, 10)  # 10 classes for MNIST\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, 32 * 28 * 28)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate Models\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Optimizers\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0001)\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daehwa/anaconda3/envs/VerificationDev/lib/python3.10/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995026/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with generator learning rate: 0.0001, discriminator learning rate: 5e-05\n",
      "Epoch[1], d_loss: 1.6818, g_loss: 1.4862, Train Accuracy: 59.81%, Test Accuracy: 79.89%\n",
      "Epoch[2], d_loss: 0.9247, g_loss: 0.2604, Train Accuracy: 81.37%, Test Accuracy: 84.67%\n",
      "Epoch[3], d_loss: 0.6680, g_loss: 0.9684, Train Accuracy: 85.06%, Test Accuracy: 86.90%\n",
      "Epoch[4], d_loss: 0.5580, g_loss: 0.8569, Train Accuracy: 86.78%, Test Accuracy: 88.37%\n",
      "Epoch[5], d_loss: 0.4961, g_loss: 0.6712, Train Accuracy: 87.81%, Test Accuracy: 88.70%\n",
      "Epoch[6], d_loss: 0.4567, g_loss: 0.5794, Train Accuracy: 88.47%, Test Accuracy: 89.54%\n",
      "Epoch[7], d_loss: 0.4272, g_loss: 0.5583, Train Accuracy: 88.95%, Test Accuracy: 89.93%\n",
      "Epoch[8], d_loss: 0.4058, g_loss: 0.5647, Train Accuracy: 89.33%, Test Accuracy: 90.17%\n",
      "Epoch[9], d_loss: 0.3894, g_loss: 0.5897, Train Accuracy: 89.61%, Test Accuracy: 90.43%\n",
      "Epoch[10], d_loss: 0.3755, g_loss: 0.5939, Train Accuracy: 89.85%, Test Accuracy: 90.69%\n",
      "Epoch[11], d_loss: 0.3639, g_loss: 0.6785, Train Accuracy: 90.10%, Test Accuracy: 90.77%\n",
      "Epoch[12], d_loss: 0.3544, g_loss: 0.6832, Train Accuracy: 90.33%, Test Accuracy: 90.89%\n",
      "Epoch[13], d_loss: 0.3465, g_loss: 0.7326, Train Accuracy: 90.49%, Test Accuracy: 91.16%\n",
      "Epoch[14], d_loss: 0.3399, g_loss: 0.6810, Train Accuracy: 90.62%, Test Accuracy: 91.12%\n",
      "Epoch[15], d_loss: 0.3334, g_loss: 0.6234, Train Accuracy: 90.78%, Test Accuracy: 91.23%\n",
      "Epoch[16], d_loss: 0.3284, g_loss: 0.6633, Train Accuracy: 90.86%, Test Accuracy: 91.32%\n",
      "Epoch[17], d_loss: 0.3238, g_loss: 0.5920, Train Accuracy: 91.01%, Test Accuracy: 91.36%\n",
      "Epoch[18], d_loss: 0.3191, g_loss: 0.4829, Train Accuracy: 91.12%, Test Accuracy: 91.45%\n",
      "Epoch[19], d_loss: 0.3155, g_loss: 0.6065, Train Accuracy: 91.24%, Test Accuracy: 91.43%\n",
      "Epoch[20], d_loss: 0.3122, g_loss: 0.6648, Train Accuracy: 91.34%, Test Accuracy: 91.50%\n",
      "Epoch[21], d_loss: 0.3096, g_loss: 0.6672, Train Accuracy: 91.35%, Test Accuracy: 91.55%\n",
      "Epoch[22], d_loss: 0.3064, g_loss: 0.6935, Train Accuracy: 91.44%, Test Accuracy: 91.52%\n",
      "Epoch[23], d_loss: 0.3035, g_loss: 0.6770, Train Accuracy: 91.51%, Test Accuracy: 91.66%\n"
     ]
    }
   ],
   "source": [
    "from RL_utils import sample_action_and_logprob, add_brightness_to_batch_images\n",
    "\n",
    "# Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Early Stopping Parameters\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "best_test_accuracy = 0.0  # Best test accuracy\n",
    "counter = 0  # Counter for non-improvement epochs\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "g_learning_rate = [0.0001, 0.0002, 0.0005, 0.001, 0.002]\n",
    "d_learning_rate = [0.00005, 0.0001, 0.0002, 0.0005, 0.001, 0.002]\n",
    "\n",
    "for g_lr in g_learning_rate:\n",
    "    for d_lr in d_learning_rate:\n",
    "        print(f\"Training with generator learning rate: {g_lr}, discriminator learning rate: {d_lr}\")\n",
    "        best_test_accuracy = 0.0\n",
    "        counter = 0\n",
    "\n",
    "        # Instantiate Models\n",
    "        generator = Generator().to(device)\n",
    "        discriminator = Discriminator().to(device)\n",
    "\n",
    "        # Optimizers\n",
    "        g_optimizer = optim.Adam(generator.parameters(), lr=g_lr)\n",
    "        d_optimizer = optim.Adam(discriminator.parameters(), lr=d_lr)\n",
    "\n",
    "        # Training Loop\n",
    "        for epoch in range(1000):  # Example Epochs\n",
    "            total_d_loss = 0\n",
    "            total_g_loss = 0\n",
    "            correct_labels = 0\n",
    "            total_labels = 0\n",
    "            \n",
    "            for i, (images, labels) in enumerate(train_loader):  # Assuming train_loader is defined and provides images and labels\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                action_mean, action_std = generator(images)\n",
    "\n",
    "                actions, log_probs = sample_action_and_logprob(action_mean, action_std)\n",
    "\n",
    "                adv_images = add_brightness_to_batch_images(images, actions)\n",
    "\n",
    "                # Zero Gradients\n",
    "                d_optimizer.zero_grad()\n",
    "\n",
    "                # Forward Pass through Discriminator\n",
    "                adv_outputs = discriminator(adv_images)\n",
    "\n",
    "                # Calculate Loss\n",
    "                d_loss = criterion(adv_outputs, labels)\n",
    "\n",
    "                # Backward Pass\n",
    "                d_loss.backward()\n",
    "                d_optimizer.step()\n",
    "\n",
    "                # Zero Gradients\n",
    "                g_optimizer.zero_grad()\n",
    "\n",
    "                g_loss = -torch.mean(log_probs) * d_loss.detach()  # Detach d_loss to avoid computing gradients\n",
    "\n",
    "                # Update Generator based on Reward\n",
    "                g_loss.backward()\n",
    "                g_optimizer.step()\n",
    "\n",
    "                total_d_loss += d_loss.item()\n",
    "                total_g_loss += g_loss.item()\n",
    "\n",
    "                # Calculate the number of correct labels\n",
    "                _, predicted = torch.max(adv_outputs, 1)\n",
    "                correct_labels += (predicted == labels).sum().item()\n",
    "                total_labels += labels.size(0)\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            train_accuracy = 100 * correct_labels / total_labels\n",
    "\n",
    "            # Calculate test accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in test_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    outputs = discriminator(images)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "            test_accuracy = 100 * correct / total\n",
    "\n",
    "            # Early Stopping\n",
    "            if test_accuracy > best_test_accuracy:\n",
    "                best_test_accuracy = test_accuracy\n",
    "                counter = 0  # Reset counter\n",
    "            else:\n",
    "                counter += 1\n",
    "\n",
    "            if counter >= patience:\n",
    "                print(\"Early Stopping triggered.\")\n",
    "                break  # Stop training\n",
    "\n",
    "            print(f\"Epoch[{epoch+1}], d_loss: {total_d_loss/len(train_loader):.4f}, g_loss: {total_g_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with learning rate: 0.0001\n",
      "Epoch[1/5], Loss: 0.4114, Train Accuracy: 88.11%, Test Accuracy: 91.05%\n",
      "Epoch[2/5], Loss: 0.3152, Train Accuracy: 90.89%, Test Accuracy: 90.42%\n",
      "Epoch[3/5], Loss: 0.2998, Train Accuracy: 91.35%, Test Accuracy: 91.58%\n",
      "Epoch[4/5], Loss: 0.2895, Train Accuracy: 91.80%, Test Accuracy: 92.34%\n",
      "Epoch[5/5], Loss: 0.2813, Train Accuracy: 92.06%, Test Accuracy: 91.25%\n",
      "Training with learning rate: 0.0002\n",
      "Epoch[1/5], Loss: 0.3919, Train Accuracy: 88.48%, Test Accuracy: 90.92%\n",
      "Epoch[2/5], Loss: 0.3095, Train Accuracy: 91.24%, Test Accuracy: 91.82%\n",
      "Epoch[3/5], Loss: 0.2932, Train Accuracy: 91.75%, Test Accuracy: 91.70%\n",
      "Epoch[4/5], Loss: 0.2837, Train Accuracy: 92.02%, Test Accuracy: 91.91%\n",
      "Epoch[5/5], Loss: 0.2778, Train Accuracy: 92.15%, Test Accuracy: 91.76%\n",
      "Training with learning rate: 0.00030000000000000003\n",
      "Epoch[1/5], Loss: 0.3885, Train Accuracy: 88.68%, Test Accuracy: 91.20%\n",
      "Epoch[2/5], Loss: 0.3062, Train Accuracy: 91.25%, Test Accuracy: 91.89%\n",
      "Epoch[3/5], Loss: 0.2902, Train Accuracy: 91.75%, Test Accuracy: 91.48%\n",
      "Epoch[4/5], Loss: 0.2820, Train Accuracy: 92.06%, Test Accuracy: 92.08%\n",
      "Epoch[5/5], Loss: 0.2750, Train Accuracy: 92.22%, Test Accuracy: 92.23%\n",
      "Training with learning rate: 0.0004\n",
      "Epoch[1/5], Loss: 0.4011, Train Accuracy: 88.09%, Test Accuracy: 91.23%\n",
      "Epoch[2/5], Loss: 0.3096, Train Accuracy: 91.24%, Test Accuracy: 91.74%\n",
      "Epoch[3/5], Loss: 0.2953, Train Accuracy: 91.71%, Test Accuracy: 91.68%\n",
      "Epoch[4/5], Loss: 0.2841, Train Accuracy: 91.98%, Test Accuracy: 91.66%\n",
      "Epoch[5/5], Loss: 0.2782, Train Accuracy: 92.14%, Test Accuracy: 92.01%\n",
      "Training with learning rate: 0.0005\n",
      "Epoch[1/5], Loss: 0.4250, Train Accuracy: 87.75%, Test Accuracy: 91.26%\n",
      "Epoch[2/5], Loss: 0.3134, Train Accuracy: 91.05%, Test Accuracy: 91.27%\n",
      "Epoch[3/5], Loss: 0.2974, Train Accuracy: 91.62%, Test Accuracy: 91.98%\n",
      "Epoch[4/5], Loss: 0.2862, Train Accuracy: 91.89%, Test Accuracy: 92.27%\n",
      "Epoch[5/5], Loss: 0.2771, Train Accuracy: 92.05%, Test Accuracy: 91.86%\n",
      "Training with learning rate: 0.0006000000000000001\n",
      "Epoch[1/5], Loss: 0.4109, Train Accuracy: 88.09%, Test Accuracy: 90.10%\n",
      "Epoch[2/5], Loss: 0.3109, Train Accuracy: 91.14%, Test Accuracy: 91.87%\n",
      "Epoch[3/5], Loss: 0.2945, Train Accuracy: 91.63%, Test Accuracy: 91.88%\n",
      "Epoch[4/5], Loss: 0.2856, Train Accuracy: 91.94%, Test Accuracy: 91.79%\n",
      "Epoch[5/5], Loss: 0.2777, Train Accuracy: 92.17%, Test Accuracy: 91.64%\n",
      "Training with learning rate: 0.0007\n",
      "Epoch[1/5], Loss: 0.4274, Train Accuracy: 87.84%, Test Accuracy: 90.98%\n",
      "Epoch[2/5], Loss: 0.3158, Train Accuracy: 91.08%, Test Accuracy: 91.94%\n",
      "Epoch[3/5], Loss: 0.2961, Train Accuracy: 91.50%, Test Accuracy: 91.87%\n",
      "Epoch[4/5], Loss: 0.2870, Train Accuracy: 91.87%, Test Accuracy: 91.53%\n",
      "Epoch[5/5], Loss: 0.2808, Train Accuracy: 91.99%, Test Accuracy: 92.32%\n",
      "Training with learning rate: 0.0008\n",
      "Epoch[1/5], Loss: 0.4523, Train Accuracy: 87.73%, Test Accuracy: 90.25%\n",
      "Epoch[2/5], Loss: 0.3241, Train Accuracy: 90.77%, Test Accuracy: 91.69%\n",
      "Epoch[3/5], Loss: 0.2993, Train Accuracy: 91.49%, Test Accuracy: 91.64%\n",
      "Epoch[4/5], Loss: 0.2878, Train Accuracy: 91.79%, Test Accuracy: 91.94%\n",
      "Epoch[5/5], Loss: 0.2827, Train Accuracy: 91.87%, Test Accuracy: 91.79%\n",
      "Training with learning rate: 0.0009000000000000001\n",
      "Epoch[1/5], Loss: 0.4692, Train Accuracy: 87.68%, Test Accuracy: 90.91%\n",
      "Epoch[2/5], Loss: 0.3198, Train Accuracy: 90.97%, Test Accuracy: 91.02%\n",
      "Epoch[3/5], Loss: 0.2994, Train Accuracy: 91.49%, Test Accuracy: 91.86%\n",
      "Epoch[4/5], Loss: 0.2898, Train Accuracy: 91.79%, Test Accuracy: 91.55%\n",
      "Epoch[5/5], Loss: 0.2812, Train Accuracy: 91.97%, Test Accuracy: 91.77%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# CNN Model Definition\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(32 * 28 * 28, 10)  # 10 classes for MNIST\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, 32 * 28 * 28)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "learning_rates = [i*0.0001 for i in range(1,10)]  # 여러 learning rate를 리스트로 정의\n",
    "\n",
    "# Data Loading and Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training Loop for each Learning Rate\n",
    "for learning_rate in learning_rates:  # 각 learning rate에 대해 반복\n",
    "    print(f\"Training with learning rate: {learning_rate}\")\n",
    "\n",
    "    # Model, Loss Function, Optimizer\n",
    "    model = CNN()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Zero Gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward Pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Compute Loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward Pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update Weights\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute Training Accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "\n",
    "        # Compute Test Accuracy\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        test_accuracy = 100 * correct / total\n",
    "\n",
    "        print(f\"Epoch[{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VerificationDev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
